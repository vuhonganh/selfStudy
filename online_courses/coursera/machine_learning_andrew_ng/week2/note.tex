This is the note of week 2 in the course Machine Learning by Andrew Ng. It covers the Multivariate Linear Regression and a tutorial of Octave. It also talks about computing parameters of problem \eqref{form:costFx} in an analytical way without proof of the method. 

\section{Multivariate Linear Regression}
\subsection{Multiple Features}
Notation
\begin{itemize}
 	\item $m$ = number of training examples
 	\item $n$ = number of features
 	\item $x^{(i)}$ = input (features) of $i^{th}$ training example
 	\item $x_{j}^{(i)}$ = value of feature j in $i^{th}$ training example
 \end{itemize} 

Hence, the hypothesis function now is $h_{\theta}(x^{(i)}) = \theta_0 + \theta_1 x_1^{(i)} + \theta_2 x_2^{(i)} + ... + \theta_n x_n^{(i)}$. For convenience of notation, we define $x_0^{(i)} = 1$ then:
\[
x^{(i)} = \begin{pmatrix}
x_0^{(i)} \\
x_1^{(i)} \\
... \\
x_n^{(i)}
\end{pmatrix} \in \Re^{n+1}  
\]
and matrix form of $\theta$ is:
\begin{align} \label{form:MatTheta}
\theta = \begin{pmatrix}
\theta_0 \\
\theta_1 \\
... \\
\theta_n
\end{pmatrix} \in \Re^{n+1}
\end{align}
\myaligns{Vector Parameters}
and
\begin{align}
\label{form:w2mulVarH}
h_\theta(x^{(i)}) = \theta^{T}x^{(i)}
\end{align}

We can rewrite them in matrix form like this:
\begin{align} \label{eq:hypo}
  h_\theta(X) = X\theta 
\end{align}
\myaligns{H function}

where $X$ is a $m \times (n+1)$ matrix of input (all features in all training examples, each column is each feature vector):
\begin{align} \label{form:matX}
X &= \begin{pmatrix}
x_0^{(1)} & x_1^{(1)} & x_2^{(1)} & ... & x_n^{(1)} \\
x_0^{(2)} & x_1^{(2)} & x_2^{(2)} & ... & x_n^{(2)} \\
...       & ...       & ...       & ... & ...\\
x_0^{(m)} & x_1^{(m)} & x_2^{(m)} & ... & x_n^{(m)}
\end{pmatrix}
\end{align}
\myaligns{Matrix of Input}

\subsection{Gradient Descent for Multiple Variables}
For $j = 0,..,n $ simultaneously update:
\begin{align}
\label{form:w2mulVarGradDesc}
\theta_j := \theta_j - \alpha \frac{1}{m} \Sigma_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}  
\end{align}
Note that $x_0^{(i)} = 1$ for $i = 1,..,m$ in the training set.

\subsection{Feature Scaling}
Feature scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step. Since the range of values of raw data varies widely, in some machine learning algorithms, especially in Gradient Descent, objective functions will not work properly without normalization. We can rescale a feature value $x_j^{(i)}$, for $j > 0$ like this (do not apply with $x_0^{(i)} = 1$):
\begin{align}
\label{form:w2FeatScale}
x_j^{(i)} := \frac{x_j^{(i)} - \Sigma_{i=1}^{m} x_j^{(i)}/m}{max_{i=1..m}(x_j^{(i)}) - min_{i=1..m}(x_j^{(i)})} 
\end{align}
\myaligns{Feature Scaling}
One question: how to prove analytically that the problem remains the same after this change?

\subsection{Learning Rate}
The learning rate $\alpha > 0$ needs to be small enough but if it's too small it will make so much time to reach the minimum. So, by experience, we can try with values like 0.003, 0.01, 0.03, 0.1, 0.3, 1, etc. (the latter equals 3 times the former).

\subsection{Polynomial Regression}
If we want to fit a relation like: $h_\theta(x) = \theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3 $ so we just turn $x^2 \rightarrow x_2$ and $x^3 \rightarrow x_3$ and we continue to compute as usual. Remember to apply Feature Scaling (formula \eqref{form:w2FeatScale}) if applicable.

\section{Computing Parameters Analytically}
The course only provided the result without a concrete proof. I will add if I have time. The idea is all partial derivatives of cost function of features equals 0: $\frac{\partial J(\theta)}{\partial \theta_j} = 0$ for $j=0,..,n$. The result is:
\begin{align} \label{form:paraAna}
\theta = (X^T X)^{-1}X^Ty
\end{align}
\myaligns{Analytical Form of $\theta$}
where $X$ is the $m \times (n+1)$ input matrix declared in \eqref{form:matX}.
and $y$ is the vector of output:
\begin{align}
y = \begin{pmatrix} \label{form:matY}
y^{(1)} \\
y^{(2)} \\
... \\
y^{(m)}
\end{pmatrix}
\end{align}
\myaligns{Vector of Output}

In Octave, to compute this, it's \textbf{pinv}(X$'$*X)*X$'$*y. For big $m$ and $n$, we need to switch from Analytical Method to Gradient Descent Method because it takes $O(n^3)$ to compute $(X^TX)^{-1}$. \\
Note that in Octave, \textbf{pinv} is pseudo-inverse and it'll give correct answer even if $X^TX$ is non-invertible. To avoid $X^TX$ is non-invertible, we can remove redundant features (like two features are size in $feet^2$ and feature size in $m^2$) and delete features if we have too many features compared to the number of training examples (i.e. $m \leq n$). 

\section{Introduction to Octave}
\subsection{Basic Operators}
Some basic operators are explained in listing \ref{lst:MatManip}. 
\begin{lstlisting}[label=lst:MatManip,caption=Basic Octave]
%% Change Octave prompt  
PS1('>> ');

%% Elementary operations
5+6
3-2
5*8
1/2
2^6
1 == 2  % false
% Not of something is ~ and not "!" 
1 ~= 2  % true
1 && 0
1 || 0
xor(1,0)

%% Variable assignment
a = 3; % semicolon suppresses output
b = 'hi';
c = 3>=1;

% Displaying them:
a = pi
disp(a)
% Use the same syntax with prinf in C
disp(printf('2 decimals: %0.2f', a))
% sprintf add a enter to a new line 
disp(sprintf('6 decimals: %0.6f', a))
format long %change the format to long (more to be printed)
a
format short %default is short
a

%% Vectors and matrices
A = [1 2; 3 4; 5 6]

v = [1 2 3]
v = [1; 2; 3]
v = [1:0.1:2]  % from 1 to 2, with stepsize of 0.1. Useful for plot axes
v = 1:6        % from 1 to 6, assumes stepsize of 1 (row vector)

C = 2*ones(2,3)  % same as C = [2 2 2; 2 2 2]
w = ones(1,3)    % 1x3 vector of ones
w = zeros(1,3)
w = rand(1,3)  % drawn from a uniform distribution 
w = randn(1,3) % drawn from a normal distribution (mean=0, var=1)
w = -6 + sqrt(10)*(randn(1,10000));  % (mean = -6, var = 10) - note: add the semicolon
hist(w)     % plot histogram using 10 bins (default)
hist(w,50)  % plot histogram using 50 bins
% note: if hist() crashes, try "graphics_toolkit('gnu_plot')" 

I = eye(4)    % 4x4 identity matrix

% Help function
help eye
help rand
help help

%% Dimensions
sz = size(A) % 1x2 matrix: [(number of rows) (number of columns)]
size(A,1)  % number of rows
size(A,2)  % number of cols
length(v)  % size of longest dimension

%% Indexing
A(3,2)  % indexing is (row,col)
A(2,:)  % get the 2nd row. 
        % ":" means every element along that dimension
A(:,2)  % get the 2nd col
A([1 3],:) % print all  the elements of rows 1 and 3

A(:,2) = [10; 11; 12]     % change second column
A = [A, [100; 101; 102]]; % append column vec
A(:) % Select all elements as a column vector.

% Putting data together 
A = [1 2; 3 4; 5 6]
B = [11 12; 13 14; 15 16] % same dims as A
C = [A B]  % concatenating A and B matrices side by side
C = [A, B] % concatenating A and B matrices side by side
C = [A; B] % Concatenating A and B top and bottom
\end{lstlisting} 

\subsection{Moving Data Around}
Two files storing data are added: \textbf{featuresX.dat} (storing zise and number of pieces of the house) and \textbf{priceY.dat} (storing the price of the house). 
\begin{lstlisting}[label=lst:dataManip,caption=Moving Data Around]
% Change directory to week2 and list files in there
ls     % result: featuresX.dat  note.tex  priceY.dat 

% Load data
load priceY.dat    % alternatively, load('priceY.dat')
load featuresX.dat
who    % list variables in workspace
whos   % like above with detailed view with size, bytes, type (double, char) 

% Unload data
clear priceY

% Take some elements from var: var(start:end)
v = priceY(1:5) % give 5 elements from 1st to 5nd

% Save variable v to a matrix file (test.mat) 
save test.mat v

% Unload all data
clear 

% When you load test.mat it will store data to variable v
load test.mat

% Save to a readable format like ASCII text
save test.txt v -ascii

% fopen, fread, fprintf, fscanf also work (not needed in class)

% Create function by creating a functionName.m containing code of this function
% For example, create a function squareandCubeThisNo.m containing:
function [y1, y2] = squareandCubeThisNo(x)
    y1 = x^2
    y2 = x^3

% To call the function, need to add its path:
addpath('/path/to/function/')

% To remember the path for future sessions of Octave,
% after executing addpath above, also do:
savepath

% Call function
functionName(args)
\end{lstlisting}

\subsection{Computing Data}
Below listing is some data computation example code.
\begin{lstlisting}[label=lst:dataComp, caption=Computing Data]
%% initialize variables
A = [1 2;3 4;5 6]
B = [11 12;13 14;15 16]
C = [1 1;2 2]
v = [1;2;3]

%% matrix operations
A * C  % matrix multiplication
A .* B % element-wise multiplication
% A .* C  or A * B gives error - wrong dimensions
A .^ 2 % element-wise square of each element in A
1./v   % element-wise reciprocal
log(v)  % functions like this operate element-wise on vecs or matrices 
exp(v)
abs(v)

-v  % -1*v

v + ones(length(v), 1)  
% v + 1  % same

A'  % matrix transpose

%% misc useful functions

% max  (or min)
a = [1 15 2 0.5]
val = max(a)
[val,ind] = max(a) % val -  maximum element of the vector a and index - index value where maximum occur
val = max(A) % if A is matrix, returns max from each column

% compare values in a matrix & find
a < 3 % checks which values in a are less than 3
find(a < 3) % gives location of elements less than 3
A = magic(3) % generates a magic matrix - not much used in ML algorithms
[r,c] = find(A>=7)  % row, column indices for values matching comparison

% sum, prod
sum(a)
prod(a)
floor(a) % or ceil(a)
max(rand(3),rand(3))
max(A,[],1) -  maximum along columns(defaults to columns - max(A,[]))
max(A,[],2) - maximum along rows
A = magic(9)
sum(A,1)
sum(A,2)
sum(sum( A .* eye(9) ))
sum(sum( A .* flipud(eye(9)) ))


% Matrix inverse (pseudo-inverse)
pinv(A)        % inv(A'*A)*A'
\end{lstlisting}

\subsection{Plotting Data}
In listing \ref{lst:dataPlot} is some example of plotting data.
\begin{lstlisting}[label=lst:dataPlot, caption= Plotting Data]
%% plotting
t = [0:0.01:0.98];
y1 = sin(2*pi*4*t); 
plot(t,y1);
y2 = cos(2*pi*4*t);
hold on;  % "hold off" to turn off
plot(t,y2,'r');
xlabel('time');
ylabel('value');
legend('sin','cos');
title('my plot');
print -dpng 'myPlot.png'
print -eps 'myPlot.eps'
close;           % or,  "close all" to close all figs
figure(1); plot(t, y1);
figure(2); plot(t, y2);
figure(2), clf;  % can specify the figure number
subplot(1,2,1);  % Divide plot into 1x2 grid, access 1st element
plot(t,y1);
subplot(1,2,2);  % Divide plot into 1x2 grid, access 2nd element
plot(t,y2);
axis([0.5 1 -1 1]);  % change axis scale

%% display a matrix (or image) 
figure;
imagesc(magic(15)), colorbar, colormap gray;
% comma-chaining function calls.  
a=1,b=2,c=3
a=1;b=2;c=3;
\end{lstlisting}


\subsection{Control Statements}
\begin{lstlisting}[label=lst:ctrlState, caption=Control Statements in Octave]
v = zeros(10,1);
for i=1:10, 
    v(i) = 2^i;
end;
% Can also use "break" and "continue" inside for and while loops to control execution.

i = 1;
while i <= 5,
  v(i) = 100; 
  i = i+1;
end

i = 1;
while true, 
  v(i) = 999; 
  i = i+1;
  if i == 6,
    break;
  end;
end

if v(1)==1,
  disp('The value is one!');
elseif v(1)==2,
  disp('The value is two!');
else
  disp('The value is not one or two!');
end
\end{lstlisting}

\section{Vectorization}
Vectorization is the process of taking code that relies on loops and converting it into matrix operations. It is more efficient, more elegant, and more concise.
As an example, let's compute our prediction from a hypothesis (see listing \ref{lst:vectorization}). Theta is the vector of fields for the hypothesis and x is a vector of variables.
\begin{lstlisting}[label=lst:vectorization, caption=Vectorization Example]
% With loops:
prediction = 0.0;
for j = 1:n+1,
  prediction += theta(j) * x(j);
end;

% With vectorization:
prediction = theta' * x;
\end{lstlisting}
We will rewrite some formulas in matrix form in order to easily write them in code. For example, the \textbf{Cost Function} can be rewritten as in \eqref{form:matCostFunc} where $X$, $y$ and $\theta$ are represented respectively in formulas \eqref{form:matX}, \eqref{form:matY} and \eqref{form:MatTheta}.
\begin{align} \label{form:matCostFunc}
J(\theta) = \frac{1}{2m} (X\theta - y)^T(X\theta - y)
\end{align}
\myaligns{Cost Function in matrix form}

Matrix form of \textbf{Gradient Descent Method} to compute parameters $\theta$ is shown in formula \eqref{form:matGradDesc}.
\begin{align} \label{form:matGradDesc}
\theta := \theta - \frac{\alpha}{m}(X^T)(X\theta - y)
\end{align}
\myaligns{Gradient Descent in matrix form}